{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ab8et3OUZftz","executionInfo":{"status":"ok","timestamp":1672380645808,"user_tz":-480,"elapsed":18468,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}},"outputId":"67c0040c-9bf2-4649-c8f5-9b77e9a8d5ef"},"id":"Ab8et3OUZftz","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"72111f94","metadata":{"id":"72111f94","outputId":"ec9fe76b-a2f2-4a85-9af8-013c7c092615","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"error","timestamp":1672380870280,"user_tz":-480,"elapsed":224476,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4505 images belonging to 5 classes.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d7c8d5f30de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Iterate over the entire dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Concatenate the images and labels to the arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       img = image_utils.load_img(\n\u001b[0m\u001b[1;32m    338\u001b[0m           \u001b[0mfilepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    392\u001b[0m       \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     raise TypeError('path should be path-like or io.BytesIO'\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","# Define the transformations to apply to the images\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","img_size=224\n","Batch_size=64\n","# Load the dataset and split it into training and test sets\n","data_generator = datagen.flow_from_directory(\n","    '/content/drive/MyDrive/Datasets/White Background/Photos Capstone',\n","    target_size=(img_size, img_size),\n","    batch_size=Batch_size,\n","    class_mode='categorical'\n",")\n","\n","# Initialize empty arrays to hold the images and labels\n","X = np.empty((0, img_size, img_size, 3))\n","y = np.empty((0, 5))\n","\n","# Iterate over the entire dataset\n","for x, label in data_generator:\n","    # Concatenate the images and labels to the arrays\n","    X = np.concatenate((X, x))\n","    y = np.concatenate((y, label))\n","    # Break the loop when all images have been processed\n","    if len(X) == data_generator.n:\n","        break\n","        \n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"id":"77663923","metadata":{"id":"77663923","executionInfo":{"status":"aborted","timestamp":1672380870282,"user_tz":-480,"elapsed":16,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["# Define the VGG model\n","model = keras.models.Sequential()\n","model.add(keras.layers.Conv2D(input_shape=(img_size,img_size,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(keras.layers.Flatten())\n","model.add(keras.layers.Dense(units=4096, activation=\"relu\"))\n","model.add(keras.layers.Dense(units=4096,activation=\"relu\"))\n","model.add(keras.layers.Dense(units=5, activation=\"softmax\"))\n","# Compile the model with the loss function and optimizer\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"09e3e180","metadata":{"id":"09e3e180","executionInfo":{"status":"aborted","timestamp":1672380870283,"user_tz":-480,"elapsed":17,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"id":"6e775a25","metadata":{"id":"6e775a25","executionInfo":{"status":"aborted","timestamp":1672380870283,"user_tz":-480,"elapsed":17,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["# Train the model and store the training history\n","history = model.fit(X_train, y_train, epochs=10, batch_size=Batch_size, validation_data=(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"fac29173","metadata":{"id":"fac29173","executionInfo":{"status":"aborted","timestamp":1672380870284,"user_tz":-480,"elapsed":18,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["# Plot the training and validation accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot the training and validation loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"4d7aba1f","metadata":{"id":"4d7aba1f","executionInfo":{"status":"aborted","timestamp":1672380870284,"user_tz":-480,"elapsed":17,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["model.save('/content/drive/MyDrive/Models/vgg_white_10_epochs.h5') "]},{"cell_type":"code","execution_count":null,"id":"d39b614b","metadata":{"id":"d39b614b","executionInfo":{"status":"aborted","timestamp":1672380870285,"user_tz":-480,"elapsed":18,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["# from keras.models import load_model\n","# model = load_model('/content/drive/MyDrive/Models/vgg_white_10_epochs.h5')"]},{"cell_type":"code","execution_count":null,"id":"74d5419e","metadata":{"id":"74d5419e","executionInfo":{"status":"aborted","timestamp":1672380870285,"user_tz":-480,"elapsed":18,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"outputs":[],"source":["import pathlib\n","class_names = (data_generator.class_indices)\n","class_names = list((k) for k,v in class_names.items())\n","img = pathlib.Path('/content/drive/MyDrive/Testing/sopa1.jpg') \n","\n","img = tf.keras.utils.load_img(\n","    img, target_size=(img_size, img_size)\n",")\n","img_array = tf.keras.utils.img_to_array(img)\n","img_array = tf.expand_dims(img_array, 0) # Create a batch\n","\n","predictions = model.predict(img_array)\n","# score = tf.nn.softmax(predictions[0])\n","\n","# # print(\n","# #     \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","# #     .format(class_names[np.argmax(score)], predictions[0])\n","# # )\n","# # print(class_names[np.argmax(score)], predictions[0])\n","# print('Name: ', class_names[np.argmax(score)], 'Conf: ', np.argmax(predictions[0]))\n","score_index = tf.argmax(predictions[0])\n","score =  predictions[0, score_index]\n","print('Class : ' , class_names[score_index], 'Conf: ', score)"]},{"cell_type":"code","source":[],"metadata":{"id":"N0RDlgcspBJQ","executionInfo":{"status":"aborted","timestamp":1672380870286,"user_tz":-480,"elapsed":19,"user":{"displayName":"uptp capstone","userId":"09467836942660736266"}}},"id":"N0RDlgcspBJQ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}